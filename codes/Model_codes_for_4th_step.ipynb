{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW5GH0rvtIDd"
      },
      "source": [
        "# Model codes for 4th step in LLM-augumented Statistical Causal Discovery\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NdB8DG3Z4bi"
      },
      "source": [
        "###Installing packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWw1GzgjQAFm"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.25.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i59CJeENtDV8"
      },
      "outputs": [],
      "source": [
        "!pip install lingam\n",
        "!pip install factor_analyzer\n",
        "!pip install igraph\n",
        "!pip install pygam\n",
        "!pip install causal-learn\n",
        "!pip install semopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUi9yQgmKt_T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import graphviz\n",
        "import lingam\n",
        "import semopy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from lingam.utils import print_causal_directions, print_dagc, make_dot, make_prior_knowledge\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n",
        "import matplotlib.image as mpimg\n",
        "import io\n",
        "from scipy.stats import norm\n",
        "from copy import deepcopy\n",
        "from itertools import combinations\n",
        "from sklearn.linear_model import LassoLarsIC, LinearRegression\n",
        "from sklearn.utils import check_array, check_scalar\n",
        "\n",
        "from causallearn.search.ConstraintBased.PC import pc\n",
        "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
        "from causallearn.graph.GraphNode import GraphNode\n",
        "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
        "\n",
        "print(\"NumPy\",  \"ver:\", np.__version__)\n",
        "print(\"Pandas\", \"ver:\", pd.__version__)\n",
        "print(\"Graphviz\",   \"ver:\", graphviz.__version__)\n",
        "print(\"LiNGAM\", \"ver:\", lingam.__version__)\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1dtUs8GbDVn"
      },
      "source": [
        "### Setting of the dataset and LLM-generated prior knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQA4nHJ6JFDY"
      },
      "source": [
        "importing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPc0JWSzJK9B"
      },
      "outputs": [],
      "source": [
        "X_row = pd.read_csv('') #read the csv file of the dataset\n",
        "X_row.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR507HCyQydo"
      },
      "outputs": [],
      "source": [
        "#standardization\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_row)\n",
        "X =pd.DataFrame(X,columns=X_row.columns)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting prior_knowledge matrices"
      ],
      "metadata": {
        "id": "hUE-nYm5klmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shared reference(Pattern 0)\n",
        "probability_X0_pattern0_df = pd.read_csv('probability_X0_pattern0_L.csv', index_col=0, header=0)\n",
        "probability_X0_pattern0 = probability_X0_pattern0_df.to_numpy()\n",
        "\n",
        "# for PC algorithm\n",
        "probability_X0_pattern1_PC_df = pd.read_csv('probability_X0_pattern1_P.csv', index_col=0, header=0)\n",
        "probability_X0_pattern2_PC_df = pd.read_csv('probability_X0_pattern2_P.csv', index_col=0, header=0)\n",
        "\n",
        "probability_X0_pattern1_PC = probability_X0_pattern1_PC_df.to_numpy()\n",
        "probability_X0_pattern2_PC = probability_X0_pattern2_PC_df.to_numpy()\n",
        "\n",
        "# for Exact Search algorithm\n",
        "probability_X0_pattern1_ExactSearch_df = pd.read_csv('probability_X0_pattern1_E.csv', index_col=0, header=0)\n",
        "probability_X0_pattern2_ExactSearch_df = pd.read_csv('probability_X0_pattern2_E.csv', index_col=0, header=0)\n",
        "\n",
        "probability_X0_pattern1_ExactSearch = probability_X0_pattern1_ExactSearch_df.to_numpy()\n",
        "probability_X0_pattern2_ExactSearch = probability_X0_pattern2_ExactSearch_df.to_numpy()\n",
        "\n",
        "# for DirectLiNGAM algorithm\n",
        "probability_X0_pattern1_LiNGAM_df = pd.read_csv('probability_X0_pattern1_L.csv', index_col=0, header=0)\n",
        "probability_X0_pattern2_LiNGAM_df = pd.read_csv('probability_X0_pattern2_L.csv', index_col=0, header=0)\n",
        "probability_X0_pattern3_LiNGAM_df = pd.read_csv('probability_X0_pattern3_L.csv', index_col=0, header=0)\n",
        "probability_X0_pattern4_LiNGAM_df = pd.read_csv('probability_X0_pattern4_L.csv', index_col=0, header=0)\n",
        "\n",
        "probability_X0_pattern1_LiNGAM = probability_X0_pattern1_LiNGAM_df.to_numpy()\n",
        "probability_X0_pattern2_LiNGAM = probability_X0_pattern2_LiNGAM_df.to_numpy()\n",
        "probability_X0_pattern3_LiNGAM = probability_X0_pattern3_LiNGAM_df.to_numpy()\n",
        "probability_X0_pattern4_LiNGAM = probability_X0_pattern4_LiNGAM_df.to_numpy()"
      ],
      "metadata": {
        "id": "T1rfoAcTksQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hEsaKYnIjHk"
      },
      "source": [
        "definition of basic function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIElI2NXg5xF"
      },
      "outputs": [],
      "source": [
        "#prior knowledge matrix generation from the probability marix for LiNGAM and PC\n",
        "def LLMprobability_to_pk(probability):\n",
        "  prior_knowledge = np.empty(probability.shape, dtype=object)\n",
        "  for i in range(prior_knowledge.shape[0]):\n",
        "    for j in range(prior_knowledge.shape[0]):\n",
        "      if i == j:\n",
        "        prior_knowledge[i, j]= -1\n",
        "      else:\n",
        "        if probability[i, j]<0.05:\n",
        "          prior_knowledge[i, j]= 0\n",
        "        elif probability[i, j]>0.95:\n",
        "          prior_knowledge[i, j]= 1\n",
        "        else:\n",
        "          prior_knowledge[i, j]= -1\n",
        "  return prior_knowledge\n",
        "\n",
        "#prior knowledge matrix generation from the probability marix for for Exact Search\n",
        "def LLMprobability_to_super_structure(probability):\n",
        "  prior_knowledge = np.empty(probability.shape, dtype=object)\n",
        "  for i in range(prior_knowledge.shape[0]):\n",
        "    for j in range(prior_knowledge.shape[0]):\n",
        "      if i == j:\n",
        "        prior_knowledge[i, j]= 0\n",
        "      else:\n",
        "        if probability[i, j]<0.05:\n",
        "          prior_knowledge[i, j]= 0\n",
        "        else:\n",
        "          prior_knowledge[i, j]= 1\n",
        "  return prior_knowledge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculation of the stats of model fitting\n",
        "def evaluate_model_fit(adjacency_matrix, X, is_ordinal=None):\n",
        "    \"\"\" evaluate the given adjacency matrix and return fit indices\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adjacency_matrix : array-like, shape (n_features, n_features)\n",
        "        Adjacency matrix representing a causal graph.\n",
        "        The i-th column and row correspond to the i-th column of X.\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training data.\n",
        "    is_ordinal : array-like, shape (n_features,)\n",
        "        Binary list. The i-th element represents that the i-th column of X is ordinal or not.\n",
        "        0 means not ordinal, otherwise ordinal.\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    fit_indices : pandas.DataFrame\n",
        "        Fit indices. This API uses semopy's calc_stats(). See semopy's reference for details.\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    adj = check_array(adjacency_matrix, force_all_finite=\"allow-nan\")\n",
        "    if adj.ndim != 2 or (adj.shape[0] != adj.shape[1]):\n",
        "        raise ValueError(\"adj must be an square matrix.\")\n",
        "\n",
        "    X = check_array(X)\n",
        "    if X.shape[1] != adj.shape[1]:\n",
        "        raise ValueError(\"X.shape[1] and adj.shape[1] must be the same.\")\n",
        "\n",
        "    if is_ordinal is None:\n",
        "        is_ordinal = np.zeros(X.shape[1])\n",
        "    else:\n",
        "        is_ordinal = check_array(is_ordinal, ensure_2d=False).flatten()\n",
        "    if is_ordinal.shape[0] != adj.shape[1]:\n",
        "        raise ValueError(\"is_ordinal.shape[0] and adj.shape[1] must be the same.\")\n",
        "\n",
        "    # build desc\n",
        "    desc = \"\"\n",
        "    eta_names = []\n",
        "\n",
        "    for i, row in enumerate(adj):\n",
        "        # exogenous\n",
        "        if np.sum(np.isnan(row)) == 0 and np.sum(np.isclose(row, 0)) == row.shape[0]:\n",
        "            continue\n",
        "\n",
        "        desc += f\"x{i:d} ~ \"\n",
        "\n",
        "        for j, elem in enumerate(row):\n",
        "            if np.isnan(elem):\n",
        "                eta_name = f\"eta_{i}_{j}\" if i < j else f\"eta_{j}_{i}\"\n",
        "                desc += f\"{eta_name} + \"\n",
        "                if eta_name not in eta_names:\n",
        "                    eta_names.append(eta_name)\n",
        "            elif not np.isclose(elem, 0):\n",
        "                desc += f\"x{j:d} + \"\n",
        "        desc = desc[:-len(\" * \")] + \"\\n\"\n",
        "\n",
        "    if len(eta_names) > 0:\n",
        "        desc += \"DEFINE(latent) \" + \" \".join(eta_names) + \"\\n\"\n",
        "\n",
        "    if sum(is_ordinal) > 0:\n",
        "        indices = np.argwhere(is_ordinal).flatten()\n",
        "\n",
        "        desc += \"DEFINE(ordinal)\"\n",
        "        for i in indices:\n",
        "            desc += f\" x{i}\"\n",
        "        desc += \"\\n\"\n",
        "\n",
        "    columns = [f\"x{i:d}\" for i in range(X.shape[1])]\n",
        "    X = pd.DataFrame(X, columns=columns)\n",
        "\n",
        "    m = semopy.Model(desc)\n",
        "    m.fit(X)\n",
        "\n",
        "    stats = semopy.calc_stats(m)\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "kezjC_1i9j1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformation into binary matrix\n",
        "def create_0or1_causal_matrix(adjacency_matrix):\n",
        "    num_nodes = adjacency_matrix.shape[0]\n",
        "    causal_0or1_matrix = np.empty(adjacency_matrix.shape, dtype = object)\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "          if i==j:\n",
        "            causal_0or1_matrix[i, j] = 0\n",
        "          else:\n",
        "            if adjacency_matrix[i, j] == 0:\n",
        "                causal_0or1_matrix[i, j] = 0\n",
        "            else:\n",
        "                causal_0or1_matrix[i, j] = 1\n",
        "\n",
        "    return causal_0or1_matrix"
      ],
      "metadata": {
        "id": "qccuuaPyzq8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTvYvyuDEMqE"
      },
      "source": [
        "# Causal discovery with PC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adjacency_matrix(cg):\n",
        "\n",
        "    num_nodes = len(cg.G.nodes)\n",
        "    adj_matrix = np.zeros((num_nodes, num_nodes),dtype=int)\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "                # i <- j\n",
        "                if cg.G.graph[i][j] == 1 and cg.G.graph[j][i] == -1:\n",
        "                    adj_matrix[i, j] = 1\n",
        "                # i -- j\n",
        "                elif cg.G.graph[i][j] == -1 and cg.G.graph[j][i] == -1:\n",
        "                    adj_matrix[i, j] = -1\n",
        "                # i <-> j\n",
        "                elif cg.G.graph[i][j] == 1 and cg.G.graph[j][i] == 1:\n",
        "                    adj_matrix[i, j] = 2\n",
        "    return adj_matrix"
      ],
      "metadata": {
        "id": "JeQrpfacqc7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_adj_matrix_to_bk(adj_matrix, data):\n",
        "\n",
        "    data_array = data.to_numpy()\n",
        "    cg_without_background_knowledge = pc(data_array, independence_test_method=\"fisherz\")  # Run PC and obtain the estimated graph (CausalGraph object)\n",
        "    nodes = cg_without_background_knowledge.G.get_nodes()\n",
        "    bk = BackgroundKnowledge()\n",
        "    num_nodes = adj_matrix.shape[0]\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "            if adj_matrix[i, j] == 1:\n",
        "                # j -> i required\n",
        "                bk.add_required_by_node(nodes[j], nodes[i])\n",
        "            elif adj_matrix[i, j] == 0:\n",
        "                #  j -> i forbidden\n",
        "                bk.add_forbidden_by_node(nodes[j], nodes[i])\n",
        "\n",
        "    return bk\n"
      ],
      "metadata": {
        "id": "Z5HexBWIq2-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wo prior knowledge\n",
        "X_array = X.to_numpy()\n",
        "pcg_wo_pk = pc(X_array, independence_test_method=\"fisherz\")\n",
        "dag_est_pc_wo_pk = create_adjacency_matrix(pcg_wo_pk)\n",
        "dag_est_pc_wo_pk"
      ],
      "metadata": {
        "id": "06dZgV8Vpy8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern 0\n",
        "X_array = X.to_numpy()\n",
        "pcg_pattern0 = pc(X_array, independence_test_method=\"fisherz\", background_knowledge = convert_adj_matrix_to_bk(LLMprobability_to_pk(probability_X0_pattern0_L), X))\n",
        "dag_est_pc_pattern0 = create_adjacency_matrix(pcg_pattern0)\n",
        "np.savetxt('adj_PC_pattern0.csv', dag_est_pc_pattern0, delimiter=',')\n",
        "model_stats_pc_pattern0 = evaluate_model_fit(dag_est_pc_pattern0, X)\n",
        "model_stats_pc_pattern0.to_csv('model_stats_PC_pattern0.csv', index=False)\n",
        "dag_est_pc_pattern0"
      ],
      "metadata": {
        "id": "MeMdwOkjq6gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pattern 1\n",
        "X_array = X.to_numpy()\n",
        "pcg_pattern1 = pc(X_array, independence_test_method=\"fisherz\", background_knowledge = convert_adj_matrix_to_bk(LLMprobability_to_pk(probability_X0_pattern1_P), X))\n",
        "dag_est_pc_pattern1 = create_adjacency_matrix(pcg_pattern1)\n",
        "np.savetxt('adj_PC_pattern1.csv', dag_est_pc_pattern1, delimiter=',')\n",
        "model_stats_pc_pattern1 = evaluate_model_fit(dag_est_pc_pattern1, X)\n",
        "model_stats_pc_pattern1.to_csv('model_stats_PC_pattern1.csv', index=False)\n",
        "dag_est_pc_pattern1"
      ],
      "metadata": {
        "id": "MVwOrw5Cqv6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pattern 2\n",
        "X_array = X.to_numpy()\n",
        "pcg_pattern2 = pc(X_array, independence_test_method=\"fisherz\", background_knowledge = convert_adj_matrix_to_bk(LLMprobability_to_pk(probability_X0_pattern2_P), X))\n",
        "dag_est_pc_pattern2 = create_adjacency_matrix(pcg_pattern2)\n",
        "np.savetxt('adj_PC_pattern2.csv', dag_est_pc_pattern2, delimiter=',')\n",
        "model_stats_pc_pattern2 = evaluate_model_fit(dag_est_pc_pattern2, X)\n",
        "model_stats_pc_pattern2.to_csv('model_stats_PC_pattern2.csv', index=False)\n",
        "dag_est_pc_pattern2"
      ],
      "metadata": {
        "id": "pRJeFJXor6e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal discovery with Exact Search"
      ],
      "metadata": {
        "id": "EHVS6WHFpnqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#wo prior knowledge\n",
        "X_array = X.to_numpy()\n",
        "dag_est_ES_wo_pk, search_stats = bic_exact_search(X_array, super_graph=None, verbose=False)\n",
        "dag_est_ES_wo_pk"
      ],
      "metadata": {
        "id": "3QiNe8_cp371"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern 0\n",
        "X_array = X.to_numpy()\n",
        "dag_est_ES_pattern0, search_stats = bic_exact_search(X_array, super_graph=LLMprobability_to_super_structure(probability_X0_pattern0_L), verbose=False)\n",
        "np.savetxt('adj_ES_pattern0.csv', dag_est_ES_pattern0, delimiter=',')\n",
        "model_stats_ES_pattern0 = evaluate_model_fit(dag_est_ES_pattern0, X)\n",
        "model_stats_ES_pattern0.to_csv('model_stats_ES_pattern0.csv', index=False)\n",
        "dag_est_ES_pattern0"
      ],
      "metadata": {
        "id": "qKJz5Uwvsuwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern 1\n",
        "X_array = X.to_numpy()\n",
        "dag_est_ES_pattern1, search_stats = bic_exact_search(X_array, super_graph=LLMprobability_to_super_structure(probability_X0_pattern1_E), verbose=False)\n",
        "np.savetxt('adj_ES_pattern1.csv', dag_est_ES_pattern1, delimiter=',')\n",
        "model_stats_ES_pattern1 = evaluate_model_fit(dag_est_ES_pattern1, X)\n",
        "model_stats_ES_pattern1.to_csv('model_stats_ES_pattern1.csv', index=False)\n",
        "dag_est_ES_pattern1"
      ],
      "metadata": {
        "id": "NzCPYyyVtBLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern 2\n",
        "X_array = X.to_numpy()\n",
        "dag_est_ES_pattern2, search_stats = bic_exact_search(X_array, super_graph=LLMprobability_to_super_structure(probability_X0_pattern2_ES), verbose=False)\n",
        "np.savetxt('adj_ES_pattern2.csv', dag_est_ES_pattern2, delimiter=',')\n",
        "model_stats_ES_pattern2 = evaluate_model_fit(dag_est_ES_pattern2, X)\n",
        "model_stats_ES_pattern2.to_csv('model_stats_ES_pattern2.csv', index=False)\n",
        "dag_est_ES_pattern2"
      ],
      "metadata": {
        "id": "lw9ci3Y2tIJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Discovery with DirectLiNGAM"
      ],
      "metadata": {
        "id": "gsPKacgTp2p1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uJBowrkJPTv"
      },
      "outputs": [],
      "source": [
        "def make_prior_knowledge_graph(prior_knowledge_matrix):\n",
        "    d = graphviz.Digraph(engine='dot')\n",
        "\n",
        "    labels = [f'x{i}' for i in range(prior_knowledge_matrix.shape[0])]\n",
        "    for label in labels:\n",
        "        d.node(label, label)\n",
        "\n",
        "    dirs = np.where(prior_knowledge_matrix > 0)\n",
        "    for to, from_ in zip(dirs[0], dirs[1]):\n",
        "        d.edge(labels[from_], labels[to])\n",
        "\n",
        "    dirs = np.where(prior_knowledge_matrix < 0)\n",
        "    for to, from_ in zip(dirs[0], dirs[1]):\n",
        "        if to != from_:\n",
        "            d.edge(labels[from_], labels[to], style='dashed')\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQX2m9L0JjaR"
      },
      "outputs": [],
      "source": [
        "#wo prior knowledge\n",
        "model_wo_pk = lingam.DirectLiNGAM(prior_knowledge=None)\n",
        "model_wo_pk.fit(X)\n",
        "dag_est_LiNGAM_wo_pk = make_dot(model_wo_pk.adjacency_matrix_)\n",
        "create_0or1_causal_matrix(model_wo_pk.adjacency_matrix_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern0\n",
        "model_pattern0 = lingam.DirectLiNGAM(prior_knowledge=LLMprobability_to_pk(probability_X0_pattern0_L))\n",
        "model_pattern0.fit(X)\n",
        "np.savetxt('adj_LiNGAM_pattern0.csv', model_pattern0.adjacency_matrix_, delimiter=',')\n",
        "model_stats_LiNGAM_pattern0 = evaluate_model_fit(model_pattern0.adjacency_matrix_, X)\n",
        "model_stats_LiNGAM_pattern0.to_csv('model_stats_LiNGAM_pattern0.csv', index=False)\n",
        "create_0or1_causal_matrix(model_pattern0.adjacency_matrix_)"
      ],
      "metadata": {
        "id": "0phJHcEDOSER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern1\n",
        "model_pattern1 = lingam.DirectLiNGAM(prior_knowledge=LLMprobability_to_pk(probability_X0_pattern1_LiNGAM))\n",
        "model_pattern1.fit(X)\n",
        "np.savetxt('adj_LiNGAM_pattern1.csv', model_pattern1.adjacency_matrix_, delimiter=',')\n",
        "model_stats_LiNGAM_pattern1 = evaluate_model_fit(model_pattern1.adjacency_matrix_, X)\n",
        "model_stats_LiNGAM_pattern1.to_csv('model_stats_LiNGAM_pattern1.csv', index=False)\n",
        "create_0or1_causal_matrix(model_pattern1.adjacency_matrix_)"
      ],
      "metadata": {
        "id": "E_1OE15KBDmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern2\n",
        "model_pattern2 = lingam.DirectLiNGAM(prior_knowledge=LLMprobability_to_pk(probability_X0_pattern2_L))\n",
        "model_pattern2.fit(X)\n",
        "np.savetxt('adj_LiNGAM_pattern2.csv', model_pattern2.adjacency_matrix_, delimiter=',')\n",
        "model_stats_LiNGAM_pattern2 = evaluate_model_fit(model_pattern2.adjacency_matrix_, X)\n",
        "model_stats_LiNGAM_pattern2.to_csv('model_stats_LiNGAM_pattern2.csv', index=False)\n",
        "create_0or1_causal_matrix(model_pattern2.adjacency_matrix_)"
      ],
      "metadata": {
        "id": "ktb_f900MyvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern3\n",
        "model_pattern3 = lingam.DirectLiNGAM(prior_knowledge=LLMprobability_to_pk(probability_X0_pattern3_L))\n",
        "model_pattern3.fit(X)\n",
        "np.savetxt('adj_LiNGAM_pattern3.csv', model_pattern3.adjacency_matrix_, delimiter=',')\n",
        "model_stats_LiNGAM_pattern3 = evaluate_model_fit(model_pattern3.adjacency_matrix_, X)\n",
        "model_stats_LiNGAM_pattern3.to_csv('model_stats_LiNGAM_pattern3.csv', index=False)\n",
        "create_0or1_causal_matrix(model_pattern3.adjacency_matrix_)"
      ],
      "metadata": {
        "id": "kyOIeiXZNBNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern4\n",
        "model_pattern4 = lingam.DirectLiNGAM(prior_knowledge=probability_X0_pattern4_L)\n",
        "model_pattern4.fit(X)\n",
        "np.savetxt('adj_LiNGAM_pattern4.csv', model_pattern4.adjacency_matrix_, delimiter=',')\n",
        "model_stats_LiNGAM_pattern4 = evaluate_model_fit(model_pattern4.adjacency_matrix_, X)\n",
        "model_stats_LiNGAM_pattern4.to_csv('model_stats_LiNGAM_pattern4.csv', index=False)\n",
        "create_0or1_causal_matrix(model_pattern4.adjacency_matrix_)"
      ],
      "metadata": {
        "id": "NoNdJCkyNO5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "epbsKYbiKERq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}